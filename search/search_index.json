{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DCR - Document Content Recognition 1. Introduction Based on the paper \"Unfolding the Structure of a Document using Deep Learning\" ( Rahman and Finin, 2019 ), this software project attempts to use various software techniques to automatically recognise the structure in any pdf documents and thus make them more searchable. The processing logic is as follows: New documents are made available in the file directory inbox . If required, other language-related file directories can also be used (see section Document Language ). Documents in a file format accepted by DCR are registered and moved to the file directory \u00ecnbox_accepted . All other documents are registered and moved to the file directory \u00ecnbox_rejected . Documents not in pdf format are converted to pdf format using Pandoc and TeX Live . Documents based on scanning which, therefore, do not contain text elements, are scanned and converted to pdf format using the Tesseract OCR software. This process applies to all image format files e.g. jpeg , tiff etc., as well as scanned images in pdf format. From all pdf documents, the text and associated metadata is extracted into a document-specific xml file using PDFlib TET . The document-specific xml files are then parsed and the DCR -relevant contents are written to the JSON files. From the JSON file(s) spaCy extracts qualified tokens and stores them either in a JSON file or in the database table token . 1.1 Rahman & Finin Paper 1.2 Supported File Types DCR can handle the following file types based on the file extension: bmp bitmap image file csv comma-separated values docx Office Open XML epub e-book file format gif Graphics Interchange Format html HyperText Markup Language jp2 JPEG 2000 jpeg Joint Photographic Experts Group odt Open Document Format for Office Applications pdf Portable Document Format png Portable Network Graphics pnm portable any-map format rst reStructuredText (RST rtf Rich Text Format tif Tag Image File Format tiff Tag Image File Format webp Image file format with lossless and lossy compression 2. Detailed Processing Actions The documents to be processed are divided into individual steps, so-called actions. Each action has the task of changing the state of a document by transforming an input file format into a different output file format. The database tables run , document , and action document the current state of a document, as well as the actions performed so far. If an error occurs during the processing of the document, this is recorded in the database tables document and action . During the next run with the same action, the faulty documents are also processed again. 2.1 Preprocessor 2.1.1 Preprocessor Architecture 2.1.2 Process the inbox directory (action: p_i ) In the first action, the file directory inbox is checked for new document files. An entry is created in the document database table for each new document, showing the current processing status of the document. The association of document and language is managed via subdirectories of the file folder inbox . In the database table language , the column directory_name_inbox specifies per language in which subdirectory the documents in this language are to be supplied. Detailed information on this can be found in the chapter Running DCR in the section Document Language . The new document files are processed based on their file extension as follows: 2.1.2.1 File extension pdf The module fitz from package PyMuPDF is used to check whether the pdf document is a scanned image or not. A pdf document consisting of a scanned image is marked for conversion from pdf format to an image format and moved to the file directory \u00ecnbox_accepted . Other pdf documents are marked for further processing with the pdf parser and then also moved to the file directory \u00ecnbox_accepted . If, however, when checking the pdf document with fitz , it turns out that the document with the file extension pdf is not really a pdf document, then the document is moved to the file directory inbox_rejected . 2.1.2.2 File extensions of documents for processing with Pandoc and TeX Live Document files with the following file extensions are moved to the file directory \u00ecnbox_accepted and marked for converting to pdf format using Pandoc and TeX Live : csv docx epub html odt rst rtf An exception are files with the file name README.md , which are ignored and not processed. 2.1.2.3 File extensions of documents for processing with Tesseract OCR Document files with the following file extensions are moved to the file directory \u00ecnbox_accepted and marked for converting to pdf format using Tesseract OCR : bmp gif jp2 jpeg png pnm tif tiff webp 2.1.2.4 Other file extensions of documents Document files that do not fall into one of the previous categories are marked as faulty and moved to the file directory \u00ecnbox_rejected . 2.1.3 Convert pdf documents to image files (action: p_2_i ) This processing action only has to be carried out if there are new pdf documents in the document input that only consist of scanned images. pdf documents consisting of scanned images must first be processed with OCR software in order to extract text they contain. Since Tesseract OCR does not support the pdf file format, such a pdf document must first be converted into one or more image files. This is done with the software pdf2image , which in turn is based on the Poppler software. The processing of the original document (parent document) is then completed and the further processing is carried out with the newly created image file(s) (child document(s)). Since an image file created here always contains only one page of a pdf document, a multi-page pdf document is distributed over several image files. After processing with Tesseract OCR , these separated files are then combined into one pdf document. 2.1.4 Convert appropriate image files to pdf files (action: ocr ) This processing action only has to be performed if there are new documents in the document entry that correspond to one of the document types listed in section 2.1.2.3. In this processing action, the documents of this document types are converted to the pdf format using Tesseract OCR . After processing with Tesseract OCR , the files split in the previous processing action are combined into a single pdf document. 2.1.5 Convert appropriate non- pdf documents to pdf files (action: n_2_p ) This processing action only has to be performed if there are new documents in the document entry that correspond to one of the document types listed in section 2.1.2.2. In this processing action, the documents of this document types are converted to pdf format using Pandoc and TeX Live . 2.2 NLP 2.2.1 NLP Architecture 2.2.2 Extract text from pdf documents (action: tet ) In this processing action, the text of the pdf documents from sections 2.1.2.1, 2.1.4 and 2.1.5 are extracted and written to xml files in tetml format for each document. The PDFlib TET library is used for this purpose. Depending on the configuration parameters tetml_page and tetml_word , up to three different xml files with different granularity can be created per document: tetml_line : granularity document line (generated by default), tetml_page : granularity document page , tetml_word : granularity document word . The page variant and the word variant are both optional. Example extract from granularity line : <Pages> <Page number=\"1\" width=\"594.96\" height=\"840.96\"> <Options>granularity=line</Options> <Content granularity=\"line\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"> <Para> <Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Line llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Text>19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm</Text> </Line> </Box> </Para> Example extract from granularity page : <Pages> <Page number=\"1\" width=\"594.96\" height=\"840.96\"> <Options>granularity=page</Options> <Content granularity=\"page\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"> <Para> <Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Text>19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm</Text> </Box> </Para> Example extract from granularity word : <Pages> <Page number=\"1\" width=\"594.96\" height=\"840.96\"> <Options>granularity=word tetml={elements={line}}</Options> <Content granularity=\"word\" dehyphenation=\"false\" dropcap=\"false\" font=\"false\" geometry=\"false\" shadow=\"false\" sub=\"false\" sup=\"false\"> <Para> <Box llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Line llx=\"26.45\" lly=\"818.96\" urx=\"485.41\" ury=\"826.96\"> <Word> <Text>19</Text> <Box llx=\"26.45\" lly=\"818.96\" urx=\"34.45\" ury=\"826.96\"/> </Word> <Word> <Text>/</Text> <Box llx=\"34.45\" lly=\"818.96\" urx=\"36.67\" ury=\"826.96\"/> </Word> <Word> <Text>04</Text> <Box llx=\"36.67\" lly=\"818.96\" urx=\"44.67\" ury=\"826.96\"/> </Word> 2.2.3 Store the parser result in a JSON file (action: s_p_j ) From the xml files of the granularity document line ( <file_name>_<doc_id>.line.xml ) or document word ( <file_name>_<doc_id>.word.xml ) created in the previous action, the text contained is now extracted with the existing metadata using xml parsing and stored in a JSON format in the database tables content_tetml_line and content_tetml_word . The document line granularity attempts to type the lines. Details on this process can be found in section 4. Example extract from granularity line : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noLinesFooter\": 1, \"noLinesHeader\": 1, \"noLinesInDocument\": 2220, \"noLinesToc\": 85, \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 829, \"noTablesInDocument\": 5, \"pages\": [ { \"pageNo\": 1, \"noLinesInPage\": 15, \"noParagraphsInPage\": 7, \"lines\": [ { \"coordLLX\": 26.45, \"coordURX\": 485.41, \"lineIndexPage\": 0, \"lineIndexParagraph\": 0, \"lineNo\": 1, \"lineType\": \"h\", \"paragraphNo\": 1, \"text\": \"19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm\" }, Example extract from the optional file line_list_bullet : { Example extract from the optional file line_list_number : { Example extract from the optional file line_table : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noTablesInDocument\": 5, \"tables\": [ { \"firstRowLLX\": 52.0, \"firstRowURX\": 426.45, \"noColumns\": 30, \"noRows\": 10, \"pageNoFrom\": 9, \"pageNoTill\": 9, \"tableNo\": 1, \"rows\": [ { \"firstColumnLLX\": 52.0, \"lastColumnURX\": 426.45, \"noColumns\": 3, \"rowNo\": 1, \"columns\": [ { \"columnNo\": 1, \"coordLLX\": 52.0, \"coordURX\": 63.77, \"lineIndexPage\": 18, \"lineIndexParagraph\": 0, \"lineNo\": 1, \"paragraphNo\": 4, \"text\": \"No.\" }, Example extract from the optional file line_toc : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"toc\": [ { \"headingLevel\": 1, \"headingText\": \"1. Lease Term: After the existing Tenant has vacated Landlord will allow Tenant to access the Demised\", \"pageNo\": 4, \"headingCtxLine1\": \"not delay or interfere with the completion of the Allowance Improvements by the Landlord in any material respect; and (b) prior to\", \"headingCtxLine2\": \"entering the Demised Premises the Tenant shall provide insurance coverage as required by this Lease. Landlord shall offer the\", \"headingCtxLine3\": \"existing tenant an early termination of its lease on December 31, 2011, instead of the normal expiration date of January 31, 2012.\", \"regexp\": \"\\\\d+\\\\.$\" }, Example extract from granularity page : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 829, \"pages\": [ { \"pageNo\": 1, \"noParagraphsInPage\": 7, \"paragraphs\": [ { \"paragraphNo\": 1, \"text\": \"19/04/2020 https://www.sec.gov/Archives/edgar/data/821002/000157104917003132/t1700141_ex10-19.htm\" }, Example extract from granularity word : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noLinesInDocument\": 2217, \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 828, \"noWordsInDocument\": 38674, \"pages\": [ { \"pageNo\": 1, \"noLinesInPage\": 15, \"noParagraphsInPage\": 7, \"noWordsInPage\": 112, \"paragraphs\": [ { \"paragraphNo\": 1, \"noLinesInParagraph\": 1, \"noWordsInParagraph\": 28, \"lines\": [ { \"lineNo\": 1, \"noWordsInLine\": 28, \"words\": [ { \"wordNo\": 1, \"text\": \"19\" }, 2.2.4 Create qualified document tokens (action: tkn ) For tokenization, spaCy is used. The document text is made available to spaCy page by page. Either the granularity document line or document page can be used for this. With the granularity document line , the recognised headers and footers are left out of the token creation. spaCy provides a number of attributes for the token. Details can be found here in the spaCy documentation. The configuration parameters of the type spacy_tkn_attr_... control which of these attributes are stored to the database table content_token . In the event of an error, the original document is marked as erroneous and an explanatory entry is also written in the document table. Example extract from granularity line : { \"documentId\": 1, \"documentFileName\": \"Example.pdf\", \"noLinesFooter\": 1, \"noLinesHeader\": 1, \"noLinesInDocument\": 2031, \"noLinesToc\": 85, \"noPagesInDocument\": 57, \"noParagraphsInDocument\": 630, \"noSentencesInDocument\": 949, \"noTablesInDocument\": 5, \"noTokensInDocument\": 16495, \"pages\": [ { \"pageNo\": 1, \"noLinesInPage\": 13, \"noParagraphsInPage\": 5, \"noSentencesInPage\": 5, \"noTokensInPage\": 39, \"paragraphs\": [ { \"paragraphNo\": 2, \"noLinesInParagraph\": 2, \"noSentencesInParagraph\": 1, \"noTokensInParagraph\": 7, \"sentences\": [ { \"sentenceNo\": 1, \"coordLLX\": 34.0, \"coordURX\": 244.18, \"lineType\": \"b\", \"noTokensInSentence\": 7, \"text\": \"EX-10.19 3 t1700141_ex10-19.htm EXHIBIT 10.19 Exhibit 10.19\", \"tokens\": [ { \"tknI\": 0, \"tknIsOov\": true, \"tknLemma_\": \"ex-10.19\", \"tknNorm_\": \"ex-10.19\", \"tknPos_\": \"NUM\", \"tknTag_\": \"CD\", \"tknText\": \"EX-10.19\", \"tknWhitespace_\": \" \" }, 3. Auxiliary File Names The processing actions are based on different flat files, each of which is generated from the original document on an action-related basis. Apart from the JSON files optionally created during the 'tokenizer' action, these can be automatically deleted after error-free processing. 3.1 Naming System Action p_i - process the inbox directory in : <ost>.<oft> out: <ost>_<di>.<oft> Action p_2_i - convert pdf documents to image files in : <ost>_<di>.pdf out: <ost>_<di>.<jpeg|png> Action ocr - convert image files to pdf documents in : <ost>_<di>.<oft> or : <ost>_<di>.<jpeg|png> out: <ost>_<di>_<pn>.pdf <ost>_<di>_0.pdf Action n_2_p - convert non-pdf documents to pdf documents in : <ost>_<di>.<oft> out: <ost>_<di>.pdf Action tet - extract text and metadata from pdf documents in : <ost>_<di>[_<pn>|_0].pdf out: <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml Action s_p_j - store the parser result in a JSON file in : <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml out: <ost>_<di>[_<pn>|_0]_line.json <ost>_<di>[_<pn>|_0]_line.list_bullet.json <ost>_<di>[_<pn>|_0]_line.list_number.json <ost>_<di>[_<pn>|_0]_line.table.json <ost>_<di>[_<pn>|_0]_line.toc.json <ost>_<di>[_<pn>|_0]_page.json <ost>_<di>[_<pn>|_0]_word.json Action tkn - create qualified document tokens in : <ost>_<di>[_<pn>|_0]_line.json out: <ost>_<di>[_<pn>|_0]_line_token.json Abbr. Meaning oft original file type osn original stem name di document identifier pn page number 3.2 Examples 3.2.1 Possible intermediate files from a docx document: case_2_docx_route_inbox_pandoc_pdflib_2.docx case_2_docx_route_inbox_pandoc_pdflib_2.pdf case_2_docx_route_inbox_pandoc_pdflib_2.line.xml case_2_docx_route_inbox_pandoc_pdflib_2.page.xml case_2_docx_route_inbox_pandoc_pdflib_2.word.xml case_2_docx_route_inbox_pandoc_pdflib_2.line.json case_2_docx_route_inbox_pandoc_pdflib_2.line_list_bullet.json case_2_docx_route_inbox_pandoc_pdflib_2.line_list_number.json case_2_docx_route_inbox_pandoc_pdflib_2.line_table.json case_2_docx_route_inbox_pandoc_pdflib_2.line_toc.json case_2_docx_route_inbox_pandoc_pdflib_2.page.json case_2_docx_route_inbox_pandoc_pdflib_2.word.json case_2_docx_route_inbox_pandoc_pdflib_2.line.token.json 3.2.2 Possible intermediate files from a jpg document: case_6_jpg_route_inbox_tesseract_pdflib_6.jpg case_6_jpg_route_inbox_tesseract_pdflib_6.pdf case_6_jpg_route_inbox_tesseract_pdflib_6.line.xml case_6_jpg_route_inbox_tesseract_pdflib_6.page.xml case_6_jpg_route_inbox_tesseract_pdflib_6.word.xml case_6_jpg_route_inbox_tesseract_pdflib_6.line.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_list_bullet.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_list_number.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_table.json case_6_jpg_route_inbox_tesseract_pdflib_6.line_toc.json case_6_jpg_route_inbox_tesseract_pdflib_6.page.json case_6_jpg_route_inbox_tesseract_pdflib_6.word.json case_6_jpg_route_inbox_tesseract_pdflib_6.line.token.json 3.2.3 Possible intermediate files from a proper pdf document: case_3_pdf_text_route_inbox_pdflib_3.pdf case_3_pdf_text_route_inbox_pdflib_3.line.xml case_3_pdf_text_route_inbox_pdflib_3.page.xml case_3_pdf_text_route_inbox_pdflib_3.word.xml case_3_pdf_text_route_inbox_pdflib_3.line.json case_3_pdf_text_route_inbox_pdflib_3.line_list_bullet.json case_3_pdf_text_route_inbox_pdflib_3.line_list_number.json case_3_pdf_text_route_inbox_pdflib_3.line_table.json case_3_pdf_text_route_inbox_pdflib_3.line_toc.json case_3_pdf_text_route_inbox_pdflib_3.page.json case_3_pdf_text_route_inbox_pdflib_3.word.json case_3_pdf_text_route_inbox_pdflib_3.line.token.json 3.2.4 Possible intermediate files from a single page scanned image pdf document: case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4.pdf case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.jpeg case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.pdf case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.xml case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.page.xml case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.word.xml case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_list_bullet.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_list_number.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_table.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line_toc.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.page.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.word.json case_4_pdf_image_small_route_inbox_pdf2image_tesseract_pdflib_4_1.line.token.json 3.2.5 Possible intermediate files from a multi page scanned image pdf document: case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_1.jpeg case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_2.jpeg case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_1.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_2.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.pdf case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.xml case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.page.xml case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.word.xml case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_list_bullet.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_list_number.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_table.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line_toc.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.page.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.word.json case_5_pdf_image_large_route_inbox_pdf2image_tesseract_pypdf2_pdflib_5_0.line.token.json","title":"Home"},{"location":"#dcr-document-content-recognition","text":"","title":"DCR - Document Content Recognition"},{"location":"#1-introduction","text":"Based on the paper \"Unfolding the Structure of a Document using Deep Learning\" ( Rahman and Finin, 2019 ), this software project attempts to use various software techniques to automatically recognise the structure in any pdf documents and thus make them more searchable. The processing logic is as follows: New documents are made available in the file directory inbox . If required, other language-related file directories can also be used (see section Document Language ). Documents in a file format accepted by DCR are registered and moved to the file directory \u00ecnbox_accepted . All other documents are registered and moved to the file directory \u00ecnbox_rejected . Documents not in pdf format are converted to pdf format using Pandoc and TeX Live . Documents based on scanning which, therefore, do not contain text elements, are scanned and converted to pdf format using the Tesseract OCR software. This process applies to all image format files e.g. jpeg , tiff etc., as well as scanned images in pdf format. From all pdf documents, the text and associated metadata is extracted into a document-specific xml file using PDFlib TET . The document-specific xml files are then parsed and the DCR -relevant contents are written to the JSON files. From the JSON file(s) spaCy extracts qualified tokens and stores them either in a JSON file or in the database table token .","title":"1. Introduction"},{"location":"#11-rahman-finin-paper","text":"","title":"1.1 Rahman &amp; Finin Paper"},{"location":"#12-supported-file-types","text":"DCR can handle the following file types based on the file extension: bmp bitmap image file csv comma-separated values docx Office Open XML epub e-book file format gif Graphics Interchange Format html HyperText Markup Language jp2 JPEG 2000 jpeg Joint Photographic Experts Group odt Open Document Format for Office Applications pdf Portable Document Format png Portable Network Graphics pnm portable any-map format rst reStructuredText (RST rtf Rich Text Format tif Tag Image File Format tiff Tag Image File Format webp Image file format with lossless and lossy compression","title":"1.2 Supported File Types"},{"location":"#2-detailed-processing-actions","text":"The documents to be processed are divided into individual steps, so-called actions. Each action has the task of changing the state of a document by transforming an input file format into a different output file format. The database tables run , document , and action document the current state of a document, as well as the actions performed so far. If an error occurs during the processing of the document, this is recorded in the database tables document and action . During the next run with the same action, the faulty documents are also processed again.","title":"2. Detailed Processing Actions"},{"location":"#21-preprocessor","text":"","title":"2.1 Preprocessor"},{"location":"#22-nlp","text":"","title":"2.2 NLP"},{"location":"#3-auxiliary-file-names","text":"The processing actions are based on different flat files, each of which is generated from the original document on an action-related basis. Apart from the JSON files optionally created during the 'tokenizer' action, these can be automatically deleted after error-free processing.","title":"3. Auxiliary File Names"},{"location":"#31-naming-system","text":"Action p_i - process the inbox directory in : <ost>.<oft> out: <ost>_<di>.<oft> Action p_2_i - convert pdf documents to image files in : <ost>_<di>.pdf out: <ost>_<di>.<jpeg|png> Action ocr - convert image files to pdf documents in : <ost>_<di>.<oft> or : <ost>_<di>.<jpeg|png> out: <ost>_<di>_<pn>.pdf <ost>_<di>_0.pdf Action n_2_p - convert non-pdf documents to pdf documents in : <ost>_<di>.<oft> out: <ost>_<di>.pdf Action tet - extract text and metadata from pdf documents in : <ost>_<di>[_<pn>|_0].pdf out: <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml Action s_p_j - store the parser result in a JSON file in : <ost>_<di>[_<pn>|_0]_line.xml <ost>_<di>[_<pn>|_0]_page.xml <ost>_<di>[_<pn>|_0]_word.xml out: <ost>_<di>[_<pn>|_0]_line.json <ost>_<di>[_<pn>|_0]_line.list_bullet.json <ost>_<di>[_<pn>|_0]_line.list_number.json <ost>_<di>[_<pn>|_0]_line.table.json <ost>_<di>[_<pn>|_0]_line.toc.json <ost>_<di>[_<pn>|_0]_page.json <ost>_<di>[_<pn>|_0]_word.json Action tkn - create qualified document tokens in : <ost>_<di>[_<pn>|_0]_line.json out: <ost>_<di>[_<pn>|_0]_line_token.json Abbr. Meaning oft original file type osn original stem name di document identifier pn page number","title":"3.1 Naming System"},{"location":"#32-examples","text":"","title":"3.2 Examples"},{"location":"code_of_conduct/","text":"DCR - Code of Conduct 1. Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. 2. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting 3. Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. 4. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. 5. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at info@konnexions.ch. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. 6. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available here . For answers to common questions about this code of conduct, see here","title":"Code of Conduct"},{"location":"code_of_conduct/#dcr-code-of-conduct","text":"","title":"DCR - Code of Conduct"},{"location":"code_of_conduct/#1-our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"1. Our Pledge"},{"location":"code_of_conduct/#2-our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"2. Our Standards"},{"location":"code_of_conduct/#3-our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"3. Our Responsibilities"},{"location":"code_of_conduct/#4-scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"4. Scope"},{"location":"code_of_conduct/#5-enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at info@konnexions.ch. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"5. Enforcement"},{"location":"code_of_conduct/#6-attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available here . For answers to common questions about this code of conduct, see here","title":"6. Attribution"},{"location":"contributing/","text":"DCR - Contributing Guide 1. License In case of software changes we strongly recommend you to respect the license terms. 2. Process fork it create your feature branch ( git checkout -b my-new-feature ) commit your changes ( git commit -am 'Add some feature' ) push to the branch ( git push origin my-new-feature ) create a new pull request Action points to be considered when adding a new database driver and / or a new programming language: README.md Release-Notes.md 3. Notes on the Software Development Process See Developing DCR here","title":"Contributing Guide"},{"location":"contributing/#dcr-contributing-guide","text":"","title":"DCR - Contributing Guide"},{"location":"contributing/#1-license","text":"In case of software changes we strongly recommend you to respect the license terms.","title":"1. License"},{"location":"contributing/#2-process","text":"fork it create your feature branch ( git checkout -b my-new-feature ) commit your changes ( git commit -am 'Add some feature' ) push to the branch ( git push origin my-new-feature ) create a new pull request Action points to be considered when adding a new database driver and / or a new programming language: README.md Release-Notes.md","title":"2. Process"},{"location":"contributing/#3-notes-on-the-software-development-process","text":"See Developing DCR here","title":"3. Notes on the Software Development Process"},{"location":"developing_code_formatting/","text":"DCR - Developing - Code Formatting The tools Black , docformatter and isort are used for formatting the programme code: Black - The uncompromising Python code formatter. docformatter - Formats docstrings to follow PEP 257 . isort - A Python utility / library to sort imports. All these tools are included in the call make format as well as in the call make dev . They can be executed individually with make black , make pydocstyle or make isort , where the recommended order is first make isort , then make black and finally make pydocstyle .","title":"Code Formatting"},{"location":"developing_code_formatting/#dcr-developing-code-formatting","text":"The tools Black , docformatter and isort are used for formatting the programme code: Black - The uncompromising Python code formatter. docformatter - Formats docstrings to follow PEP 257 . isort - A Python utility / library to sort imports. All these tools are included in the call make format as well as in the call make dev . They can be executed individually with make black , make pydocstyle or make isort , where the recommended order is first make isort , then make black and finally make pydocstyle .","title":"DCR - Developing - Code Formatting"},{"location":"developing_coding_standards/","text":"DCR - Developing - Coding Standards 1. Python The PEP 8 style guide for Python code is strictly applied and enforced with static analysis tools. All program code must be commented with type hinting instructions. All functions, modules and packages must be commented with Docstring . The program code must be covered as far as possible with appropriate tests - the aim is always 100 % test coverage. The successful execution of make dev ensures that the program code meets the required standards. 2. Scripts Scripts must always be available in identical functionality for both the Unix shell bash and the Windows command interpreter cmd.exe . The most important dynamic parameters of a script should be requested from the user in a dialogue. In the event of an error, the execution of the script must be terminated immediately. Apart from the main scripts, all other scripts should be present in the scripts file directory. The main scripts are: run_dcr_dev - Running the DCR functionality for development purposes. run_dcr_prod - Performing the DCR functionality for productive operation.","title":"Coding Standards"},{"location":"developing_coding_standards/#dcr-developing-coding-standards","text":"","title":"DCR - Developing - Coding Standards"},{"location":"developing_coding_standards/#1-python","text":"The PEP 8 style guide for Python code is strictly applied and enforced with static analysis tools. All program code must be commented with type hinting instructions. All functions, modules and packages must be commented with Docstring . The program code must be covered as far as possible with appropriate tests - the aim is always 100 % test coverage. The successful execution of make dev ensures that the program code meets the required standards.","title":"1. Python"},{"location":"developing_coding_standards/#2-scripts","text":"Scripts must always be available in identical functionality for both the Unix shell bash and the Windows command interpreter cmd.exe . The most important dynamic parameters of a script should be requested from the user in a dialogue. In the event of an error, the execution of the script must be terminated immediately. Apart from the main scripts, all other scripts should be present in the scripts file directory. The main scripts are: run_dcr_dev - Running the DCR functionality for development purposes. run_dcr_prod - Performing the DCR functionality for productive operation.","title":"2. Scripts"},{"location":"developing_continouos_delivery/","text":"DCR - Developing - Continuous Delivery The GitHub Actions are used to enforce the following good practices of the software engineering process in the CI/CD process: uniform formatting of all source code, static source code analysis, execution of the software testing framework, and creation of up-to-date user documentation. The action standards in the GitHub Actions guarantees compliance with the required standards, the action test_production ensures error-free compilation for production use and the action test_development runs the tests against various operating system and Python versions. The actions test_development and test_production must be able to run error-free on operating system Ubuntu 22.04 and with Python version 3.10 , the action standards is only required error-free for the latest versions of Ubuntu and Python . The individual steps to be carried out in the action standards are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code format the code with isort, Black and docformatter lint the code with Bandit, Flake8, Mypy and Pylint check the API docs with pydocstyle create and upload the user docs with Pydoc-Markdown and Mkdocs install Pandoc , Poppler , Tesseract OCR and TeX Live publish the code coverage results to coveralls.io in the action test_development are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program in the action test_production are: set up Python , pip and pipenv install the production specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program","title":"Continouos Delivery"},{"location":"developing_continouos_delivery/#dcr-developing-continuous-delivery","text":"The GitHub Actions are used to enforce the following good practices of the software engineering process in the CI/CD process: uniform formatting of all source code, static source code analysis, execution of the software testing framework, and creation of up-to-date user documentation. The action standards in the GitHub Actions guarantees compliance with the required standards, the action test_production ensures error-free compilation for production use and the action test_development runs the tests against various operating system and Python versions. The actions test_development and test_production must be able to run error-free on operating system Ubuntu 22.04 and with Python version 3.10 , the action standards is only required error-free for the latest versions of Ubuntu and Python . The individual steps to be carried out in the action standards are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code format the code with isort, Black and docformatter lint the code with Bandit, Flake8, Mypy and Pylint check the API docs with pydocstyle create and upload the user docs with Pydoc-Markdown and Mkdocs install Pandoc , Poppler , Tesseract OCR and TeX Live publish the code coverage results to coveralls.io in the action test_development are: set up Python , pip and pipenv install the development specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program in the action test_production are: set up Python , pip and pipenv install the production specific packages with pipenv compile the Python code install Pandoc , Poppler , Tesseract OCR and TeX Live run pytest for writing better program","title":"DCR - Developing - Continuous Delivery"},{"location":"developing_data_model/","text":"DCR - Developing - Data Model 1. Overview Data storage is realised with the relational database management system PostgreSQL . DCR uses the official Docker image as provided by the PostgreSQL Docker Community on DockerHub - see here . If required, a PostgreSQL Docker image can be downloaded and a PostgreSQL Docker container can be created both with the script scripts/run_setup_postgresql . 2. Database Schema 2.1 Entity-relationship (ER) Diagram 2.2 Database Table action The database table documents all actions that have been performed on the documents. Example rows : Example columns : ER Diagram : 2.3 Database Table document The database table documents the current status of the document. Example rows : Example columns : ER Diagram : 2.4 Database Table language This database table controls the language-related document properties during processing. Example rows : Example columns : ER Diagram : 2.5 Database Table run This database table documents the executed processing runs in detail. Example rows : Example columns : ER Diagram : 2.6 Database Table token This database table contains the tokens determined by spaCy together with selected attributes. Example rows : Example columns : ER Diagram : 2.7 Database Table version This database table is used to monitor the version status of the DCR database schema. Example row : Example column : ER Diagram :","title":"Data Model"},{"location":"developing_data_model/#dcr-developing-data-model","text":"","title":"DCR - Developing - Data Model"},{"location":"developing_data_model/#1-overview","text":"Data storage is realised with the relational database management system PostgreSQL . DCR uses the official Docker image as provided by the PostgreSQL Docker Community on DockerHub - see here . If required, a PostgreSQL Docker image can be downloaded and a PostgreSQL Docker container can be created both with the script scripts/run_setup_postgresql .","title":"1. Overview"},{"location":"developing_data_model/#2-database-schema","text":"","title":"2. Database Schema"},{"location":"developing_data_model/#21-entity-relationship-er-diagram","text":"","title":"2.1 Entity-relationship (ER) Diagram"},{"location":"developing_data_model/#22-database-table-action","text":"The database table documents all actions that have been performed on the documents. Example rows : Example columns : ER Diagram :","title":"2.2 Database Table action"},{"location":"developing_data_model/#23-database-table-document","text":"The database table documents the current status of the document. Example rows : Example columns : ER Diagram :","title":"2.3 Database Table document"},{"location":"developing_data_model/#24-database-table-language","text":"This database table controls the language-related document properties during processing. Example rows : Example columns : ER Diagram :","title":"2.4 Database Table language"},{"location":"developing_data_model/#25-database-table-run","text":"This database table documents the executed processing runs in detail. Example rows : Example columns : ER Diagram :","title":"2.5 Database Table run"},{"location":"developing_data_model/#26-database-table-token","text":"This database table contains the tokens determined by spaCy together with selected attributes. Example rows : Example columns : ER Diagram :","title":"2.6 Database Table token"},{"location":"developing_data_model/#27-database-table-version","text":"This database table is used to monitor the version status of the DCR database schema. Example row : Example column : ER Diagram :","title":"2.7 Database Table version"},{"location":"developing_development_environment/","text":"DCR - Developing - Development Environment To set up a suitable development environment under Ubuntu 22.04 LTS , on the one hand a suitable ready-made Docker image is provided and on the other hand two scripts to create the development system in a standalone system, a virtual environment or the Windows Subsystem for Linux (WSL2) are available. 1. Docker Image The ready-made Docker images are available on DockerHub under the following link: dcr_dev - Document Content Recognition Development Image When selecting the Docker image, care must be taken to select the appropriate version of the Docker image. 2. Script-based Solution Alternatively, for a Ubuntu 22.04 LTS environment that is as unspoiled as possible, the following two scripts are available in the scripts file directory: scripts/0.9.8/run_install_4-vm_wsl2_1.sh scripts/0.9.8/run_install_4-vm_wsl2_2.sh After a cd scripts command in a terminal window, the script run_install_4-vm_wsl2_1.sh must first be executed. Administration rights ( sudo ) are required for this. Afterwards, the second script run_install_4-vm_wsl2_2.sh must be executed in a new terminal window.","title":"Development Environment"},{"location":"developing_development_environment/#dcr-developing-development-environment","text":"To set up a suitable development environment under Ubuntu 22.04 LTS , on the one hand a suitable ready-made Docker image is provided and on the other hand two scripts to create the development system in a standalone system, a virtual environment or the Windows Subsystem for Linux (WSL2) are available.","title":"DCR - Developing - Development Environment"},{"location":"developing_development_environment/#1-docker-image","text":"The ready-made Docker images are available on DockerHub under the following link: dcr_dev - Document Content Recognition Development Image When selecting the Docker image, care must be taken to select the appropriate version of the Docker image.","title":"1. Docker Image"},{"location":"developing_development_environment/#2-script-based-solution","text":"Alternatively, for a Ubuntu 22.04 LTS environment that is as unspoiled as possible, the following two scripts are available in the scripts file directory: scripts/0.9.8/run_install_4-vm_wsl2_1.sh scripts/0.9.8/run_install_4-vm_wsl2_2.sh After a cd scripts command in a terminal window, the script run_install_4-vm_wsl2_1.sh must first be executed. Administration rights ( sudo ) are required for this. Afterwards, the second script run_install_4-vm_wsl2_2.sh must be executed in a new terminal window.","title":"2. Script-based Solution"},{"location":"developing_software_documentation/","text":"DCR - Developing - Software Documentation 1. API Documentation The creation of API documentation for functions, modules and packages is mandatory and enforced with the static analysis tool pydocstyle . pydocstyle is a static analysis tool for checking compliance with Python Docstring conventions. pydocstyle can be executed individually with make pydocstyle and is also included in both calls make docs and make dev . The Docstring format used in DCR is that of type Google . For Visual Studio Code, the extension VSCode Python Docstring Generator can be used when creating API documentation. With the Pydoc-Markdown tool, the API documentation is extracted from the source files and put into Markdown format. In this format, the API documentation can then be integrated into the user documentation. 2. Examples for the format of the API documentation Package Documentation : Package libs: DCR libraries. Module Documentation : Module pp.inbox: Check and distribute incoming documents. New documents are made available in the file directory inbox. These are then checked and moved to the accepted or rejected file directories depending on the result of the check. Depending on the file format, the accepted documents are then converted into the pdf file format either with the help of Pandoc and TeX Live or with the help of Tesseract OCR. Function Documentation : Load the command line arguments into memory.Pandoc and TeX Live The command line arguments define the process steps to be executed. The valid arguments are: all - Run the complete processing of all new documents. db_c - Create the database. db_u - Upgrade the database. n_2_p - Convert non-pdf docuents to pdf documents. ocr - Convert image docuents to pdf documents. p_i - Process the inbox directory. p_2_i - Convert pdf documents to image files. tet - Extract text from pdf documents. With the option all, the following process steps are executed in this order: 1. p_i 2. p_2_i 3. n_2_p 4. ocr 5. tet Args: argv (List[str]): Command line arguments. Returns: dict[str, bool]: The processing steps based on CLI arguments. In Visual Studio Code, the VSCode Python Docstring Generator tool can be used to create a framework for API documentation. 3. User Documentation The remaining documents for the user documentation can be found in the file directory docs in Markdown format: File Headline Remarks code_of_conduct.md Code of Conduct contributing.md Contributing Guide dcr_api.md API Documentation development.md Development Notes on the software development process index.md Document Content Recognition Background, installation and user guide license.md Text of the licence agreement release_history.md Release History Previous release notes release_notes.md Release Notes Release notes of the current version research.md Research Reference to the relevant research papers The MkDocs tool is used to create the user documentation. With the command make mkdocs the user documentation is created by MkDocs and uploaded to the GitHub pages of the repository. The command make mkdocs is also included in the calls make docs and make dev .","title":"Software Documentation"},{"location":"developing_software_documentation/#dcr-developing-software-documentation","text":"","title":"DCR - Developing - Software Documentation"},{"location":"developing_software_documentation/#1-api-documentation","text":"The creation of API documentation for functions, modules and packages is mandatory and enforced with the static analysis tool pydocstyle . pydocstyle is a static analysis tool for checking compliance with Python Docstring conventions. pydocstyle can be executed individually with make pydocstyle and is also included in both calls make docs and make dev . The Docstring format used in DCR is that of type Google . For Visual Studio Code, the extension VSCode Python Docstring Generator can be used when creating API documentation. With the Pydoc-Markdown tool, the API documentation is extracted from the source files and put into Markdown format. In this format, the API documentation can then be integrated into the user documentation.","title":"1. API Documentation"},{"location":"developing_software_documentation/#2-examples-for-the-format-of-the-api-documentation","text":"Package Documentation : Package libs: DCR libraries. Module Documentation : Module pp.inbox: Check and distribute incoming documents. New documents are made available in the file directory inbox. These are then checked and moved to the accepted or rejected file directories depending on the result of the check. Depending on the file format, the accepted documents are then converted into the pdf file format either with the help of Pandoc and TeX Live or with the help of Tesseract OCR. Function Documentation : Load the command line arguments into memory.Pandoc and TeX Live The command line arguments define the process steps to be executed. The valid arguments are: all - Run the complete processing of all new documents. db_c - Create the database. db_u - Upgrade the database. n_2_p - Convert non-pdf docuents to pdf documents. ocr - Convert image docuents to pdf documents. p_i - Process the inbox directory. p_2_i - Convert pdf documents to image files. tet - Extract text from pdf documents. With the option all, the following process steps are executed in this order: 1. p_i 2. p_2_i 3. n_2_p 4. ocr 5. tet Args: argv (List[str]): Command line arguments. Returns: dict[str, bool]: The processing steps based on CLI arguments. In Visual Studio Code, the VSCode Python Docstring Generator tool can be used to create a framework for API documentation.","title":"2. Examples for the format of the API documentation"},{"location":"developing_software_documentation/#3-user-documentation","text":"The remaining documents for the user documentation can be found in the file directory docs in Markdown format: File Headline Remarks code_of_conduct.md Code of Conduct contributing.md Contributing Guide dcr_api.md API Documentation development.md Development Notes on the software development process index.md Document Content Recognition Background, installation and user guide license.md Text of the licence agreement release_history.md Release History Previous release notes release_notes.md Release Notes Release notes of the current version research.md Research Reference to the relevant research papers The MkDocs tool is used to create the user documentation. With the command make mkdocs the user documentation is created by MkDocs and uploaded to the GitHub pages of the repository. The command make mkdocs is also included in the calls make docs and make dev .","title":"3. User Documentation"},{"location":"developing_software_testing/","text":"DCR - Developing - Software Testing pytest is used as a software testing framework with the following plugins:: pytest-cov for coverage reporting, pytest-deadfixture to list unused or duplicate fixtures, and pytest-random-order to randomise the order of the tests. On the one hand, the tests must be as complete as possible, i.e. a test coverage of 100% is aimed for, but on the other hand, the scope of the test code should be minimal, i.e. unnecessary repetitions must be strictly avoided. The best strategy for this is to first create a test case for the normal case and then add special tests for the special cases not yet covered. Finally, the tool Coveralls for Python is used to enable a connection to Coveralls .","title":"Software Testing"},{"location":"developing_software_testing/#dcr-developing-software-testing","text":"pytest is used as a software testing framework with the following plugins:: pytest-cov for coverage reporting, pytest-deadfixture to list unused or duplicate fixtures, and pytest-random-order to randomise the order of the tests. On the one hand, the tests must be as complete as possible, i.e. a test coverage of 100% is aimed for, but on the other hand, the scope of the test code should be minimal, i.e. unnecessary repetitions must be strictly avoided. The best strategy for this is to first create a test case for the normal case and then add special tests for the special cases not yet covered. Finally, the tool Coveralls for Python is used to enable a connection to Coveralls .","title":"DCR - Developing - Software Testing"},{"location":"developing_static_code_analysis/","text":"DCR - Developing - Static Code Analysis The tools Bandit , Flake8 , Mypy and Pylint are used for static code analysis: Bandit - Bandit is a tool designed to find common security issues in Python code. Flake8 - A Python tool that glues together pycodestyle , Pyflakes , McCabe , and third-party plugins to check the style and quality of some Python code. mypy - Optional static typing for Python . Pylint - It's not just a linter that annoys you! All these tools are included in the call make lint as well as in the call make dev . They can be executed individually with make bandit , make flake8 , make mypy and make pylint . Flake8 includes the following tools: McCabe - McCabe complexity checker for Python . pycodestyle - Simple Python style checker in one Python file. Pyflakes - A simple program which checks Python source files for errors. Radon - Various code metrics for Python code.","title":"Static Code Analysis"},{"location":"developing_static_code_analysis/#dcr-developing-static-code-analysis","text":"The tools Bandit , Flake8 , Mypy and Pylint are used for static code analysis: Bandit - Bandit is a tool designed to find common security issues in Python code. Flake8 - A Python tool that glues together pycodestyle , Pyflakes , McCabe , and third-party plugins to check the style and quality of some Python code. mypy - Optional static typing for Python . Pylint - It's not just a linter that annoys you! All these tools are included in the call make lint as well as in the call make dev . They can be executed individually with make bandit , make flake8 , make mypy and make pylint . Flake8 includes the following tools: McCabe - McCabe complexity checker for Python . pycodestyle - Simple Python style checker in one Python file. Pyflakes - A simple program which checks Python source files for errors. Radon - Various code metrics for Python code.","title":"DCR - Developing - Static Code Analysis"},{"location":"developing_system_environment/","text":"DCR - Developing - System Environment DCR is developed on the operating systems Ubuntu 22.04 LTS and Microsoft Windows 10 . Ubuntu is used here via the VM Workstation Player 16 . Ubuntu can also be used in conjunction with the Windows Subsystem for Linux (WSL2) . The GitHub actions for continuous integration run on Ubuntu 22.04 . Version 3.10 is used for the Python programming language.","title":"System Environment"},{"location":"developing_system_environment/#dcr-developing-system-environment","text":"DCR is developed on the operating systems Ubuntu 22.04 LTS and Microsoft Windows 10 . Ubuntu is used here via the VM Workstation Player 16 . Ubuntu can also be used in conjunction with the Windows Subsystem for Linux (WSL2) . The GitHub actions for continuous integration run on Ubuntu 22.04 . Version 3.10 is used for the Python programming language.","title":"DCR - Developing - System Environment"},{"location":"developing_version_planning/","text":"DCR - Developing - Version Planning 1. Version Planning 1.1 Open Version Feature(s) 0.9.8 TBD 1.2 Already implemented Version Feature(s) 0.9.7 Documentation and test improvements 0.9.6 Extracting an API 0.9.3 Extending NLP capabilities 0.9.2 Refactoring database and code 0.9.1 Core text preprocessing and wrangling 0.9.0 Parser 0.8.0 PDFlib TET processing 0.7.0 Tesseract OCR processing 0.6.5 Pandoc processing 0.6.0 pdf for Tesseract OCR processing 0.5.0 Inbox processing 2. Next Development Steps 2.1 Open 2.1.1 High Priority pandoc_dcr: convert doc documents to docx user: reconstruct original document 2.1.2 Normal Priority API documentation: Content improvement API documentation: Layout improvement admin: reset a list of documents: clean up the database before the next process retry - delete existing data tool: check the content of the file directory against the database line type header & footer: optional: ignore the first / last page optional: logging of the applied method optional: page number alternating in the first and last token optional: page number always in the first / last token optional: use of Levenshtein algorithm optional: use of language-related regular expressions to determine the header / footer with the page number line type table: check the coordinates table with page break 2.1.3 Low Priority Google Styleguide implementation 2.2 Already implemented API Documentation PDFlib TET processing Tesseract OCR - Installation all: database table 'document': new column file_size all: database table 'document': new column no_pages_pdf all: merge database table 'journal' into 'document' clean up the auxiliary files in file directory inbox_accepted - keep the base document combine pdf files - scanned pdf documents - after Tesseract OCR convert the appropriate documents into the pdf format with Pandoc and TeX Live duplicate handling error correction version 0.9.0 error handling - highly defensive inbox.py - process_inbox() - processing ocr & non-ocr in the same method introduce default language introduce document language - eventually inbox sub folder per language load initialisation data optionally save the original document in the database parser result with JSON parser: classify the lines, e.g. body, footer, header etc. replace TeX Live by LuaLaTeX or XeLuTeX (Unicode) test cases for file duplicate","title":"Version Planning"},{"location":"developing_version_planning/#dcr-developing-version-planning","text":"","title":"DCR - Developing - Version Planning"},{"location":"developing_version_planning/#1-version-planning","text":"","title":"1. Version Planning"},{"location":"developing_version_planning/#11-open","text":"Version Feature(s) 0.9.8 TBD","title":"1.1 Open"},{"location":"developing_version_planning/#12-already-implemented","text":"Version Feature(s) 0.9.7 Documentation and test improvements 0.9.6 Extracting an API 0.9.3 Extending NLP capabilities 0.9.2 Refactoring database and code 0.9.1 Core text preprocessing and wrangling 0.9.0 Parser 0.8.0 PDFlib TET processing 0.7.0 Tesseract OCR processing 0.6.5 Pandoc processing 0.6.0 pdf for Tesseract OCR processing 0.5.0 Inbox processing","title":"1.2 Already implemented"},{"location":"developing_version_planning/#2-next-development-steps","text":"","title":"2. Next Development Steps"},{"location":"developing_version_planning/#21-open","text":"","title":"2.1 Open"},{"location":"developing_version_planning/#22-already-implemented","text":"API Documentation PDFlib TET processing Tesseract OCR - Installation all: database table 'document': new column file_size all: database table 'document': new column no_pages_pdf all: merge database table 'journal' into 'document' clean up the auxiliary files in file directory inbox_accepted - keep the base document combine pdf files - scanned pdf documents - after Tesseract OCR convert the appropriate documents into the pdf format with Pandoc and TeX Live duplicate handling error correction version 0.9.0 error handling - highly defensive inbox.py - process_inbox() - processing ocr & non-ocr in the same method introduce default language introduce document language - eventually inbox sub folder per language load initialisation data optionally save the original document in the database parser result with JSON parser: classify the lines, e.g. body, footer, header etc. replace TeX Live by LuaLaTeX or XeLuTeX (Unicode) test cases for file duplicate","title":"2.2 Already implemented"},{"location":"license/","text":"DCR - License Konnexions Public License (KX-PL) Version 2020.05, May 2020 pdf Version This license governs use of the accompanying software. If you use the software, you accept this license. If you do not accept the license, do not use the software. Definitions. The terms \"reproduce\", \"reproduction\", \"derivative works\", and \"distribution\" have the same meaning here as under U.S. copyright law. A \"contribution\" is the original software, or any additions or changes to the software. A \"contributor\" is any person that distributes its contribution under this license. \"Licensed patents\" are a contributor's patent claims that read directly on its contribution. Grant of Rights (a) Copyright Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free copyright license to reproduce its contribution, prepare derivative works of its contribution, and distribute its contribution or any derivative works that you create. (b) Patent Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free license under its licensed patents to make, have made, use, sell, offer for sale, import, and/or otherwise dispose of its contribution in the software or derivative works of the contribution in the software. Conditions and Limitations (a) No Trademark License - This license does not grant you rights to use any contributors' name, logo, or trademarks. (b) If you bring a patent claim against any contributor over patents that you claim are infringed by the software, your patent license from such contributor to the software ends automatically.0 (c) If you distribute any portion of the software, you must retain all copyright, patent, trademark, and attribution notices that are present in the software. (d) If you distribute any portion of the software in source code form, you may do so only under this license by including a complete copy of this license with your distribution. If you distribute any portion of the software in compiled or object code form, you may only do so under a license that complies with this license. (e) The software is licensed \"as-is.\" You bear the risk of using it. The contributors give no express warranties, guarantees or conditions. You may have additional consumer rights under your local laws which this license cannot change. To the extent permitted under your local laws, the contributors exclude the implied warranties of merchantability, fitness for a particular purpose and non-infringement. (f) Source code usage under this License is limited to review, compilation and contributions. Contributions to Konnexions software products under this License may only be made in consultation with Konnexions GmbH and through the appropriate Konnexions software repositories.","title":"License"},{"location":"license/#dcr-license","text":"Konnexions Public License (KX-PL) Version 2020.05, May 2020 pdf Version This license governs use of the accompanying software. If you use the software, you accept this license. If you do not accept the license, do not use the software. Definitions. The terms \"reproduce\", \"reproduction\", \"derivative works\", and \"distribution\" have the same meaning here as under U.S. copyright law. A \"contribution\" is the original software, or any additions or changes to the software. A \"contributor\" is any person that distributes its contribution under this license. \"Licensed patents\" are a contributor's patent claims that read directly on its contribution. Grant of Rights (a) Copyright Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free copyright license to reproduce its contribution, prepare derivative works of its contribution, and distribute its contribution or any derivative works that you create. (b) Patent Grant - Subject to the terms of this license, including the license conditions and limitations in section 3, each contributor grants you a non- exclusive, worldwide, royalty-free license under its licensed patents to make, have made, use, sell, offer for sale, import, and/or otherwise dispose of its contribution in the software or derivative works of the contribution in the software. Conditions and Limitations (a) No Trademark License - This license does not grant you rights to use any contributors' name, logo, or trademarks. (b) If you bring a patent claim against any contributor over patents that you claim are infringed by the software, your patent license from such contributor to the software ends automatically.0 (c) If you distribute any portion of the software, you must retain all copyright, patent, trademark, and attribution notices that are present in the software. (d) If you distribute any portion of the software in source code form, you may do so only under this license by including a complete copy of this license with your distribution. If you distribute any portion of the software in compiled or object code form, you may only do so under a license that complies with this license. (e) The software is licensed \"as-is.\" You bear the risk of using it. The contributors give no express warranties, guarantees or conditions. You may have additional consumer rights under your local laws which this license cannot change. To the extent permitted under your local laws, the contributors exclude the implied warranties of merchantability, fitness for a particular purpose and non-infringement. (f) Source code usage under this License is limited to review, compilation and contributions. Contributions to Konnexions software products under this License may only be made in consultation with Konnexions GmbH and through the appropriate Konnexions software repositories.","title":"DCR - License"},{"location":"release_history/","text":"DCR - Release History Version 0.9.7 Release Date: 08.09.2022 1 Modified Features Delimitation of the documentation to the DCR application Delimitation of the tests to the DCR application Updating the third party software used 2 Applied Software Software Version Remark Status DBeaver 22.2.0 for virtual machine only [optional] upgrade Docker Desktop 20.10.17 base version [Docker Image & VM] Git 2.34.1 base version upgrade Pandoc 2.19.2 upgrade PFlib TET 5.3 Poppler 22.02.0 upgrade Python3 3.10.7 upgrade Tesseract OCR 5.2.0-22-g0daf1 base version upgrade TeX Live 2022 base version upgrade 2.1 Unix-specific Software Software Version Remark Status asdf v0.10.2 base version (optional) cURL 7.81.0 base version upgrade dos2unix 7.4.2 base version upgrade GCC & G++ 11.2.0 base version upgrade GNU Autoconf 2.71 base version upgrade GNU Automake 1.16.5 base version upgrade GNU make 4.3 base version upgrade htop 3.2.1 optional OpenSSL 1.1.1o upgrade procps 3.3.17 base version (optional) upgrade tmux 3.3a optional Ubuntu 22.04.4 LTS base version upgrade Vim 8.2.3995 base version (optional) upgrade Wget 1.21.2 upgrade 2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.9.6 Release Date: 07.08.2022 1 New Features API documentation added Determination of bulleted lists. Determination of numbered lists. Determination of headings. 2 Modified Features Code refactoring. 3 Applied Software Software Version Remark Status DBeaver 22.1.0 for virtual machine only [optional] Docker Desktop 20.10.17 base version [Docker Image & VM] Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.6 upgrade Python3 - pip 22.1.2 Tesseract OCR 5.1.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 3.1 Unix-specific Software Software Version Remark Status asdf v0.10.2-7e7a1fa base version (optional) cURL 7.68.0 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.2.1 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.3a optional Ubuntu 20.04.4 LTS base version Vim 8.1.3741 base version (optional) Wget 1.20.3 3.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.9.3 Release Date: 17.06.2022 1 New Features Description of the algorithms for determining the line type. Determination of the lines belonging to the TOC (Table of Content). 2 Modified Features Major refactoring of the tokenizer. pylint: Adjustments for latest version. 3 Applied Software Software Version Remark Status DBeaver 22.1.0 for virtual machine only [optional] upgrade Docker Desktop 20.10.17 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.5 upgrade Python3 - pip 22.1.2 upgrade Tesseract OCR 5.1.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 3.1 Unix-specific Software Software Version Remark Status asdf v0.10.2-7e7a1fa base version (optional) upgrade cURL 7.68.0 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.2.1 optional upgrade OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.3a optional upgrade Ubuntu 20.04.4 LTS base version upgrade Vim 8.1.3741 base version (optional) Wget 1.20.3 3.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.9.2 Release Date: 01.06.2022 1 New Features object-oriented design. selectable spaCy token attributes completed. 2 Modified Features database schema refactored 3 Applied Software Software Version Remark Status DBeaver 22.0.5 for virtual machine only [optional] upgrade Docker Desktop 20.10.16 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.4 Python3 - pip 22.0.4 Tesseract OCR 5.1.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 3.1 Unix-specific Software Software Version Remark Status asdf v0.10.1-711ad99 base version (optional) upgrade cURL 7.68.0 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.2.0 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.4 LTS base version upgrade Vim 8.1.3741 base version (optional) Wget 1.20.3 3.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version 1.4 Open issues Microsoft Windows Server 2019: (see here ) MkApi: (see here ) Tesseract OCR: (see here ) Version 0.9.1 Release Date: 05.05.2022 1. New Features classification of lines into headers, footers and body lines support for documents in different languages - English, French, German and Italian as standard tokenizer based on spaCy 2. Modified Features extending the parser to the granularities page, line and word refactoring to separate preprocessor and NLP specific processes 3. Applied Software Software Version Remark Status DBeaver 22.0.4 for virtual machine only [optional] upgrade Docker Desktop 20.10.14 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 upgrade PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.4 upgrade Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 3.1 Unix-specific Software Software Version Remark Status asdf v0.10.0-a9caa5b base version (optional) upgrade cURL 7.6.80 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.2.0 optional upgrade OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.4 LTS base version Vim 8.1.3741 base version (optional) upgrade Wget 1.20.3 3.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.9.0 Release Date: 06.04.2022 1. New Features support for documents in different languages - English, French, German and Italian as standard 2. Applied Software Software Version Remark Status DBeaver 22.0.2 for virtual machine only [optional] upgrade Docker Desktop 20.10.14 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 upgrade PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.4 upgrade Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 2.1 Unix-specific Software Software Version Remark Status asdf v0.9.0-e0d27e6 base version (optional) cURL 7.6.80 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.4 LTS base version Vim 8.1.2269 base version (optional) Wget 1.20.3 2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.8.0 Release Date: 18.03.2022 1. New Features processing step tet : Extract text from pdf files. 2. Applied Software Software Version Remark Status DBeaver 22.0.0 for virtual machine only [optional] Docker Desktop 20.10.13 base version [Docker Image & VM] Git 2.25.1 base version Pandoc 2.17.1.1 PFlib TET 5.3 new Poppler 0.86.1 base version Python3 3.10.3 upgrade Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 2.1 Unix-specific Software Software Version Remark Status asdf v0.9.0-e0d27e6 base version (optional) cURL 7.6.80 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.4 LTS base version Vim 8.1.2269 base version (optional) Wget 1.20.3 2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.7.0 Release Date: 15.03.2022 1. New Features processing step ocr : Convert appropriate image files to pdf files. 2. Applied Software Software Version Remark Status DBeaver 22.0.0 for virtual machine only [optional] Docker Desktop 20.10.13 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.17.1.1 Poppler 0.86.1 base version Python3 3.10.2 Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version new TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version 2.1 Unix-specific Software Software Version Remark Status asdf v0.9.0-e0d27e6 base version (optional) cURL 7.6.80 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.4 LTS base version Vim 8.1.2269 base version (optional) Wget 1.20.3 2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.6.5 Release Date: 10.03.2022 1. New Features processing step n_2_p : Convert appropriate non-pdf documents to pdf files. 2. Applied Software Software Version Remark Status DBeaver 22.0.0 for virtual machine only [optional] upgrade Docker Desktop 20.10.12 base version [Docker Image & VM] Git 2.25.1 base version Pandoc 2.17.1.1 new Poppler 0.86.1 base version Python3 3.10.2 Python3 - pip 22.0.4 upgrade TeX Live 2019 base version new TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version new 2.1 Unix-specific Software Software Version Remark Status asdf v0.9.0-e0d27e6 base version (optional) cURL 7.6.80 base version dos2unix 7.4.0 base version GCC & G++ 9.4.0 base version upgrade GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.4 LTS base version upgrade Vim 8.1.2269 base version (optional) Wget 1.20.3 2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version Version 0.6.0 Release Date: 04.03.2022 1. New Features Processing step db_u : Upgrade the database. Processing step p_2_i : Convert pdf documents into image files. 2. Applied Software Software Version Remark Status DBeaver 21.3.5 for virtual machine only [optional] Docker Desktop 20.10.12 base version [Docker Image & VM] new Git 2.25.1 base version Poppler 0.86.1 base version new Python3 3.10.2 Python3 - pip 22.0.3 2.1 Unix-specific Software Software Version Remark Status asdf v0.9.0-e0d27e6 base version (optional) cURL 7.6.80 base version dos2unix 7.4.0 base version GCC & G++ 9.3.0 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 optional OpenSSL 1.1.1f base version procps 3.3.16 base version (optional) tmux 3.2a optional Ubuntu 20.04.3 LTS base version Vim 8.1.3741 base version (optional) Wget 1.20.3 2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version new Make for Windows 3.81 base version new sed for Windows 4.2.1 base version new Version 0.5.0 Release Date: 14.02.2022 1. New Features Setup of the entire development infrastructure Creation of the first version of the user documentation Processing of new document arrivals in the file directory \u00ecnbox 2. Applied Software Software Version Remark asdf v0.9.0-e0d27e6 base version cURL 7.6.80 base version DBeaver 21.3.4 for virtual machine only dos2unix 7.4.0 base version GCC & G++ 9.3.0 base version Git 2.25.1 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 OpenSSL 1.1.1f base version procps-ng 3.3.16 base version Python3 3.10.2 Python3 - pip 22.0.3 tmux 3.2a Ubuntu 20.04.3 LTS base version Vim 8.1.3741 base version Wget 1.20.3","title":"Release History"},{"location":"release_history/#dcr-release-history","text":"","title":"DCR - Release History"},{"location":"release_history/#version-097","text":"Release Date: 08.09.2022","title":"Version 0.9.7"},{"location":"release_history/#1-modified-features","text":"Delimitation of the documentation to the DCR application Delimitation of the tests to the DCR application Updating the third party software used","title":"1 Modified Features"},{"location":"release_history/#2-applied-software","text":"Software Version Remark Status DBeaver 22.2.0 for virtual machine only [optional] upgrade Docker Desktop 20.10.17 base version [Docker Image & VM] Git 2.34.1 base version upgrade Pandoc 2.19.2 upgrade PFlib TET 5.3 Poppler 22.02.0 upgrade Python3 3.10.7 upgrade Tesseract OCR 5.2.0-22-g0daf1 base version upgrade TeX Live 2022 base version upgrade","title":"2 Applied Software"},{"location":"release_history/#version-096","text":"Release Date: 07.08.2022","title":"Version 0.9.6"},{"location":"release_history/#1-new-features","text":"API documentation added Determination of bulleted lists. Determination of numbered lists. Determination of headings.","title":"1 New Features"},{"location":"release_history/#2-modified-features","text":"Code refactoring.","title":"2 Modified Features"},{"location":"release_history/#3-applied-software","text":"Software Version Remark Status DBeaver 22.1.0 for virtual machine only [optional] Docker Desktop 20.10.17 base version [Docker Image & VM] Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.6 upgrade Python3 - pip 22.1.2 Tesseract OCR 5.1.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"3 Applied Software"},{"location":"release_history/#version-093","text":"Release Date: 17.06.2022","title":"Version 0.9.3"},{"location":"release_history/#1-new-features_1","text":"Description of the algorithms for determining the line type. Determination of the lines belonging to the TOC (Table of Content).","title":"1 New Features"},{"location":"release_history/#2-modified-features_1","text":"Major refactoring of the tokenizer. pylint: Adjustments for latest version.","title":"2 Modified Features"},{"location":"release_history/#3-applied-software_1","text":"Software Version Remark Status DBeaver 22.1.0 for virtual machine only [optional] upgrade Docker Desktop 20.10.17 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.5 upgrade Python3 - pip 22.1.2 upgrade Tesseract OCR 5.1.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"3 Applied Software"},{"location":"release_history/#version-092","text":"Release Date: 01.06.2022","title":"Version 0.9.2"},{"location":"release_history/#1-new-features_2","text":"object-oriented design. selectable spaCy token attributes completed.","title":"1 New Features"},{"location":"release_history/#2-modified-features_2","text":"database schema refactored","title":"2 Modified Features"},{"location":"release_history/#3-applied-software_2","text":"Software Version Remark Status DBeaver 22.0.5 for virtual machine only [optional] upgrade Docker Desktop 20.10.16 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.4 Python3 - pip 22.0.4 Tesseract OCR 5.1.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"3 Applied Software"},{"location":"release_history/#14-open-issues","text":"Microsoft Windows Server 2019: (see here ) MkApi: (see here ) Tesseract OCR: (see here )","title":"1.4 Open issues"},{"location":"release_history/#version-091","text":"Release Date: 05.05.2022","title":"Version 0.9.1"},{"location":"release_history/#1-new-features_3","text":"classification of lines into headers, footers and body lines support for documents in different languages - English, French, German and Italian as standard tokenizer based on spaCy","title":"1. New Features"},{"location":"release_history/#2-modified-features_3","text":"extending the parser to the granularities page, line and word refactoring to separate preprocessor and NLP specific processes","title":"2. Modified Features"},{"location":"release_history/#3-applied-software_3","text":"Software Version Remark Status DBeaver 22.0.4 for virtual machine only [optional] upgrade Docker Desktop 20.10.14 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 upgrade PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.4 upgrade Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"3. Applied Software"},{"location":"release_history/#version-090","text":"Release Date: 06.04.2022","title":"Version 0.9.0"},{"location":"release_history/#1-new-features_4","text":"support for documents in different languages - English, French, German and Italian as standard","title":"1. New Features"},{"location":"release_history/#2-applied-software_1","text":"Software Version Remark Status DBeaver 22.0.2 for virtual machine only [optional] upgrade Docker Desktop 20.10.14 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.18 upgrade PFlib TET 5.3 Poppler 0.86.1 base version Python3 3.10.4 upgrade Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"2. Applied Software"},{"location":"release_history/#version-080","text":"Release Date: 18.03.2022","title":"Version 0.8.0"},{"location":"release_history/#1-new-features_5","text":"processing step tet : Extract text from pdf files.","title":"1. New Features"},{"location":"release_history/#2-applied-software_2","text":"Software Version Remark Status DBeaver 22.0.0 for virtual machine only [optional] Docker Desktop 20.10.13 base version [Docker Image & VM] Git 2.25.1 base version Pandoc 2.17.1.1 PFlib TET 5.3 new Poppler 0.86.1 base version Python3 3.10.3 upgrade Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"2. Applied Software"},{"location":"release_history/#version-070","text":"Release Date: 15.03.2022","title":"Version 0.7.0"},{"location":"release_history/#1-new-features_6","text":"processing step ocr : Convert appropriate image files to pdf files.","title":"1. New Features"},{"location":"release_history/#2-applied-software_3","text":"Software Version Remark Status DBeaver 22.0.0 for virtual machine only [optional] Docker Desktop 20.10.13 base version [Docker Image & VM] upgrade Git 2.25.1 base version Pandoc 2.17.1.1 Poppler 0.86.1 base version Python3 3.10.2 Python3 - pip 22.0.4 Tesseract OCR 5.10.0 base version new TeX Live 2019 base version TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version","title":"2. Applied Software"},{"location":"release_history/#version-065","text":"Release Date: 10.03.2022","title":"Version 0.6.5"},{"location":"release_history/#1-new-features_7","text":"processing step n_2_p : Convert appropriate non-pdf documents to pdf files.","title":"1. New Features"},{"location":"release_history/#2-applied-software_4","text":"Software Version Remark Status DBeaver 22.0.0 for virtual machine only [optional] upgrade Docker Desktop 20.10.12 base version [Docker Image & VM] Git 2.25.1 base version Pandoc 2.17.1.1 new Poppler 0.86.1 base version Python3 3.10.2 Python3 - pip 22.0.4 upgrade TeX Live 2019 base version new TeX Live - pdfTeX 3.14159265-2.6-1.40.20 base version new","title":"2. Applied Software"},{"location":"release_history/#version-060","text":"Release Date: 04.03.2022","title":"Version 0.6.0"},{"location":"release_history/#1-new-features_8","text":"Processing step db_u : Upgrade the database. Processing step p_2_i : Convert pdf documents into image files.","title":"1. New Features"},{"location":"release_history/#2-applied-software_5","text":"Software Version Remark Status DBeaver 21.3.5 for virtual machine only [optional] Docker Desktop 20.10.12 base version [Docker Image & VM] new Git 2.25.1 base version Poppler 0.86.1 base version new Python3 3.10.2 Python3 - pip 22.0.3","title":"2. Applied Software"},{"location":"release_history/#version-050","text":"Release Date: 14.02.2022","title":"Version 0.5.0"},{"location":"release_history/#1-new-features_9","text":"Setup of the entire development infrastructure Creation of the first version of the user documentation Processing of new document arrivals in the file directory \u00ecnbox","title":"1. New Features"},{"location":"release_history/#2-applied-software_6","text":"Software Version Remark asdf v0.9.0-e0d27e6 base version cURL 7.6.80 base version DBeaver 21.3.4 for virtual machine only dos2unix 7.4.0 base version GCC & G++ 9.3.0 base version Git 2.25.1 base version GNU Autoconf 2.69 base version GNU Automake 1.16.1 base version GNU make 4.2.1 base version htop 3.1.2 OpenSSL 1.1.1f base version procps-ng 3.3.16 base version Python3 3.10.2 Python3 - pip 22.0.3 tmux 3.2a Ubuntu 20.04.3 LTS base version Vim 8.1.3741 base version Wget 1.20.3","title":"2. Applied Software"},{"location":"release_notes/","text":"DCR - Release Notes 1. Version 0.9.8 Release Date: dd.mm.2022 1.1 Modified Features Delimitation of the documentation to the DCR application Delimitation of the tests to the DCR application Updating the third party software used 1.2 Applied Software Software Version Remark Status DBeaver 22.2.0 for virtual machine only [optional] Docker Desktop 20.10.17 base version [Docker Image & VM] Git 2.34.1 base version Pandoc 2.19.2 PFlib TET 5.3 Poppler 22.02.0 Python3 3.10.7 Tesseract OCR 5.2.0-22-g0daf1 base version TeX Live 2022 base version 1.2.1 Unix-specific Software Software Version Remark Status asdf v0.10.2 base version (optional) cURL 7.81.0 base version dos2unix 7.4.2 base version GCC & G++ 11.2.0 base version GNU Autoconf 2.71 base version GNU Automake 1.16.5 base version GNU make 4.3 base version htop 3.2.1 optional OpenSSL 1.1.1o procps 3.3.17 base version (optional) tmux 3.3a optional Ubuntu 22.04.4 LTS base version Vim 8.2.3995 base version (optional) Wget 1.21.2 1.2.2 Windows-specific Software Software Version Remark Status Grep for Windows 2.5.4 base version Make for Windows 3.81 base version sed for Windows 4.2.1 base version 1.3 Open issues Tesseract OCR: (see here ) 2. Detailed Open Issues 2.2 Tesseract OCR Issue: Images of type 'jp2': Error in pixReadStreamJp2k: version 2.3.0: differs from minor = 2 ... see #57 Issue (ocr): Converting the file 'D:\\SoftDevelopment\\Projects\\dcr\\data\\inbox_accepted\\pdf_scanned_03_ok_5.jp2' to the file 'D:\\SoftDevelopment\\Projects\\dcr\\data\\inbox_accepted\\pdf_scanned_03_ok_5.pdf' with Tesseract OCR failed - error status: '1' - error: 'Error in pixReadStreamJp2k: version 2.3.0: differs from minor = 2 Error in pixReadStream: jp2: no pix returned Error in pixRead: pix not read Error during processing.'.","title":"Release Notes"},{"location":"release_notes/#dcr-release-notes","text":"","title":"DCR - Release Notes"},{"location":"release_notes/#1-version-098","text":"Release Date: dd.mm.2022","title":"1. Version 0.9.8"},{"location":"release_notes/#11-modified-features","text":"Delimitation of the documentation to the DCR application Delimitation of the tests to the DCR application Updating the third party software used","title":"1.1 Modified Features"},{"location":"release_notes/#12-applied-software","text":"Software Version Remark Status DBeaver 22.2.0 for virtual machine only [optional] Docker Desktop 20.10.17 base version [Docker Image & VM] Git 2.34.1 base version Pandoc 2.19.2 PFlib TET 5.3 Poppler 22.02.0 Python3 3.10.7 Tesseract OCR 5.2.0-22-g0daf1 base version TeX Live 2022 base version","title":"1.2 Applied Software"},{"location":"release_notes/#13-open-issues","text":"Tesseract OCR: (see here )","title":"1.3 Open issues"},{"location":"release_notes/#2-detailed-open-issues","text":"","title":"2. Detailed Open Issues"},{"location":"release_notes/#22-tesseract-ocr","text":"Issue: Images of type 'jp2': Error in pixReadStreamJp2k: version 2.3.0: differs from minor = 2 ... see #57 Issue (ocr): Converting the file 'D:\\SoftDevelopment\\Projects\\dcr\\data\\inbox_accepted\\pdf_scanned_03_ok_5.jp2' to the file 'D:\\SoftDevelopment\\Projects\\dcr\\data\\inbox_accepted\\pdf_scanned_03_ok_5.pdf' with Tesseract OCR failed - error status: '1' - error: 'Error in pixReadStreamJp2k: version 2.3.0: differs from minor = 2 Error in pixReadStream: jp2: no pix returned Error in pixRead: pix not read Error during processing.'.","title":" 2.2 Tesseract OCR"},{"location":"running_configuration/","text":"DCR - Running - Configuration 1. data/initial_database_dat.json This file contains the initial values relating to the database table language . The existing entries can be modified or deleted, but new entries can also be added. Syntax : { \"apiVersion\": \"9.9.9\", \"data\": { \"tables\": [ { \"tableName\": \"language\", \"rows\": [ { \"row\": [ { \"columnName\": \"active\", \"columnValue\": true | false }, { \"columnName\": \"code_iso_639_3\", \"columnValue\": \"deu\" }, { \"columnName\": \"code_spacy\", \"columnValue\": \"...\" }, { \"columnName\": \"code_tesseract\", \"columnValue\": \"...\" }, { \"columnName\": \"directory_name_inbox\", \"columnValue\": null | \"...\" }, { \"columnName\": \"iso_language_name\", \"columnValue\": \"...\" } ] }, ... ] } ] } } Example entry for a language : { \"row\": [ { \"columnName\": \"active\", \"columnValue\": false }, { \"columnName\": \"code_iso_639_3\", \"columnValue\": \"fra\" }, { \"columnName\": \"code_spacy\", \"columnValue\": \"fr_dep_news_trf\" }, { \"columnName\": \"code_tesseract\", \"columnValue\": \"fra\" }, { \"columnName\": \"directory_name_inbox\", \"columnValue\": null }, { \"columnName\": \"iso_language_name\", \"columnValue\": \"French\" } ] }, 2. logging_cfg.yaml This file controls the logging behaviour of the application. Default content : version: 1 formatters: simple: format: \"%(asctime)s %(pathname)-80s ] %(levelname)-5s %(funcName)s:%(lineno)d %(message)s\" extended: format: \"%(asctime)s %(pathname)-80s ] %(levelname)-5s %(funcName)s:%(lineno)d \\n%(message)s\" handlers: console: class: logging.StreamHandler level: INFO formatter: simple file_handler: class: logging.FileHandler level: INFO filename: logging_dcr.log formatter: extended loggers: dcr: handlers: [ console ] root: handlers: [ file_handler ] 3. setup.cfg This file controls the behaviour of the DCR application. The customisable entries are: [dcr] db_connection_port = 5432 db_connection_prefix = postgresql+psycopg2:// db_container_port = 5432 db_database = dcr_db_prod db_database_admin = dcr_db_prod_admin db_dialect = postgresql db_host = localhost db_initial_data_file = data/db_initial_data_file.json db_password = postgresql db_password_admin = postgresql db_schema = dcr_schema db_user = dcr_user db_user_admin = dcr_user_admin directory_inbox_accepted = data/inbox_prod_accepted directory_inbox_rejected = data/inbox_prod_rejected doc_id_in_file_name = none ignore_duplicates = false Parameter Default value Description db_connection_port environment specific Port number the DBMS server is listening on. db_connection_prefix postgresql+psycopg2:// Front part of the database URL. db_database environment specific DCR database name. db_database_admin environment specific Administrative database name. db_dialect postgresql DBMS used, currently: only PostgreSQL allowed. db_host localhost Host name of the DBMS server. db_initial_data_file data/db_initial_data_file.json File with initial database contents. db_password postgresql DCR database user password. db_password_admin postgresql Administrative database password. db_schema dcr_schema Database schema name. db_user postgresql DCR database user name. db_user_admin postgresql Administrative database user name. directory_inbox_accepted data/inbox_prod_accepted Directory for the accepted documents. directory_inbox_rejected data/inbox_prod_rejected Complete file name for the JSON file with the database initialisation data. doc_id_in_file_name none Position of the document id in the file name : after , before or none . ignore_duplicates false Accept presumably duplicated documents based on a SHA256 hash key. The configuration parameters can be set differently for the individual environments ( dev , prod and test ). Examples : [dcr.env.dev] db_connection_port = 5433 db_database = dcr_db_dev db_database_admin = dcr_db_dev_admin db_initial_data_file = data/db_initial_data_file_dev.json directory_inbox_accepted = data/inbox_dev_accepted directory_inbox_rejected = data/inbox_dev_rejected ...","title":"Configuration"},{"location":"running_configuration/#dcr-running-configuration","text":"","title":"DCR - Running - Configuration"},{"location":"running_configuration/#1-datainitial_database_datjson","text":"This file contains the initial values relating to the database table language . The existing entries can be modified or deleted, but new entries can also be added. Syntax : { \"apiVersion\": \"9.9.9\", \"data\": { \"tables\": [ { \"tableName\": \"language\", \"rows\": [ { \"row\": [ { \"columnName\": \"active\", \"columnValue\": true | false }, { \"columnName\": \"code_iso_639_3\", \"columnValue\": \"deu\" }, { \"columnName\": \"code_spacy\", \"columnValue\": \"...\" }, { \"columnName\": \"code_tesseract\", \"columnValue\": \"...\" }, { \"columnName\": \"directory_name_inbox\", \"columnValue\": null | \"...\" }, { \"columnName\": \"iso_language_name\", \"columnValue\": \"...\" } ] }, ... ] } ] } } Example entry for a language : { \"row\": [ { \"columnName\": \"active\", \"columnValue\": false }, { \"columnName\": \"code_iso_639_3\", \"columnValue\": \"fra\" }, { \"columnName\": \"code_spacy\", \"columnValue\": \"fr_dep_news_trf\" }, { \"columnName\": \"code_tesseract\", \"columnValue\": \"fra\" }, { \"columnName\": \"directory_name_inbox\", \"columnValue\": null }, { \"columnName\": \"iso_language_name\", \"columnValue\": \"French\" } ] },","title":"1. data/initial_database_dat.json"},{"location":"running_configuration/#2-logging_cfgyaml","text":"This file controls the logging behaviour of the application. Default content : version: 1 formatters: simple: format: \"%(asctime)s %(pathname)-80s ] %(levelname)-5s %(funcName)s:%(lineno)d %(message)s\" extended: format: \"%(asctime)s %(pathname)-80s ] %(levelname)-5s %(funcName)s:%(lineno)d \\n%(message)s\" handlers: console: class: logging.StreamHandler level: INFO formatter: simple file_handler: class: logging.FileHandler level: INFO filename: logging_dcr.log formatter: extended loggers: dcr: handlers: [ console ] root: handlers: [ file_handler ]","title":"2. logging_cfg.yaml"},{"location":"running_configuration/#3-setupcfg","text":"This file controls the behaviour of the DCR application. The customisable entries are: [dcr] db_connection_port = 5432 db_connection_prefix = postgresql+psycopg2:// db_container_port = 5432 db_database = dcr_db_prod db_database_admin = dcr_db_prod_admin db_dialect = postgresql db_host = localhost db_initial_data_file = data/db_initial_data_file.json db_password = postgresql db_password_admin = postgresql db_schema = dcr_schema db_user = dcr_user db_user_admin = dcr_user_admin directory_inbox_accepted = data/inbox_prod_accepted directory_inbox_rejected = data/inbox_prod_rejected doc_id_in_file_name = none ignore_duplicates = false Parameter Default value Description db_connection_port environment specific Port number the DBMS server is listening on. db_connection_prefix postgresql+psycopg2:// Front part of the database URL. db_database environment specific DCR database name. db_database_admin environment specific Administrative database name. db_dialect postgresql DBMS used, currently: only PostgreSQL allowed. db_host localhost Host name of the DBMS server. db_initial_data_file data/db_initial_data_file.json File with initial database contents. db_password postgresql DCR database user password. db_password_admin postgresql Administrative database password. db_schema dcr_schema Database schema name. db_user postgresql DCR database user name. db_user_admin postgresql Administrative database user name. directory_inbox_accepted data/inbox_prod_accepted Directory for the accepted documents. directory_inbox_rejected data/inbox_prod_rejected Complete file name for the JSON file with the database initialisation data. doc_id_in_file_name none Position of the document id in the file name : after , before or none . ignore_duplicates false Accept presumably duplicated documents based on a SHA256 hash key. The configuration parameters can be set differently for the individual environments ( dev , prod and test ). Examples : [dcr.env.dev] db_connection_port = 5433 db_database = dcr_db_dev db_database_admin = dcr_db_dev_admin db_initial_data_file = data/db_initial_data_file_dev.json directory_inbox_accepted = data/inbox_dev_accepted directory_inbox_rejected = data/inbox_dev_rejected ...","title":"3. setup.cfg"},{"location":"running_document_language/","text":"DCR - Running - Document Language 1. Overview DCR supports the processing of documents in different languages. The supported languages must be accepted by Pandoc respectively Babel , spaCy and Tesseract OCR . Furthermore, for each of the languages in question there must be a corresponding entry in the database table language . 2. Database Table language The active languages in the database table language control the allocation of the documents to a language. Each document language must have its own entry in this table. The documents in a particular language are expected in the subdirectory to the inbox or as defined in the directory_name_inbox column. The entry for the standard language English is created automatically when the database is set up. In the JSON file db_initial_data_file , the languages German, French and Italian are also predefined but in the inactive state. The chosen document languages must now either be activated in this file or, if not yet available, added here. Example entry for the document language French: { \"row\": [ { \"columnName\": \"active\", \"columnValue\": false }, { \"columnName\": \"code_iso_639_3\", \"columnValue\": \"fra\" }, { \"columnName\": \"code_pandoc\", \"columnValue\": \"fr\" }, { \"columnName\": \"code_spacy\", \"columnValue\": \"fr_dep_news_trf\" }, { \"columnName\": \"code_tesseract\", \"columnValue\": \"fra\" }, { \"columnName\": \"directory_name_inbox\", \"columnValue\": null }, { \"columnName\": \"iso_language_name\", \"columnValue\": \"French\" } ] }, Column Description active active language - true or false code_iso_639_3 three-letter codes, the same as 639-2/T for languages, but with distinct codes for each variety of an ISO 639 macrolanguage code_pandoc the language code as used in Pandoc respectively Babel - IETF BCP 47 } code_spacy the trained pipeline package as used in spaCy code_tesseract the language code as used in Tesseract OCR directory_name_inbox optional the name of the file directory relative to the inbox - if missing the content of the column iso_language_name is used iso_language_name the name of the language according to ISO 639-1 3. Default Document Language The default document language is English. The corresponding entry in the database table language is created internally with the following contents: Column Content active true code_iso_639_3 eng code_pandoc en code_spacy en_core_web_trf code_tesseract eng directory_name_inbox the inbox directory iso_language_name English","title":"Document Language"},{"location":"running_document_language/#dcr-running-document-language","text":"","title":"DCR - Running - Document Language"},{"location":"running_document_language/#1-overview","text":"DCR supports the processing of documents in different languages. The supported languages must be accepted by Pandoc respectively Babel , spaCy and Tesseract OCR . Furthermore, for each of the languages in question there must be a corresponding entry in the database table language .","title":"1. Overview"},{"location":"running_document_language/#2-database-table-language","text":"The active languages in the database table language control the allocation of the documents to a language. Each document language must have its own entry in this table. The documents in a particular language are expected in the subdirectory to the inbox or as defined in the directory_name_inbox column. The entry for the standard language English is created automatically when the database is set up. In the JSON file db_initial_data_file , the languages German, French and Italian are also predefined but in the inactive state. The chosen document languages must now either be activated in this file or, if not yet available, added here. Example entry for the document language French: { \"row\": [ { \"columnName\": \"active\", \"columnValue\": false }, { \"columnName\": \"code_iso_639_3\", \"columnValue\": \"fra\" }, { \"columnName\": \"code_pandoc\", \"columnValue\": \"fr\" }, { \"columnName\": \"code_spacy\", \"columnValue\": \"fr_dep_news_trf\" }, { \"columnName\": \"code_tesseract\", \"columnValue\": \"fra\" }, { \"columnName\": \"directory_name_inbox\", \"columnValue\": null }, { \"columnName\": \"iso_language_name\", \"columnValue\": \"French\" } ] }, Column Description active active language - true or false code_iso_639_3 three-letter codes, the same as 639-2/T for languages, but with distinct codes for each variety of an ISO 639 macrolanguage code_pandoc the language code as used in Pandoc respectively Babel - IETF BCP 47 } code_spacy the trained pipeline package as used in spaCy code_tesseract the language code as used in Tesseract OCR directory_name_inbox optional the name of the file directory relative to the inbox - if missing the content of the column iso_language_name is used iso_language_name the name of the language according to ISO 639-1","title":"2. Database Table language"},{"location":"running_document_language/#3-default-document-language","text":"The default document language is English. The corresponding entry in the database table language is created internally with the following contents: Column Content active true code_iso_639_3 eng code_pandoc en code_spacy en_core_web_trf code_tesseract eng directory_name_inbox the inbox directory iso_language_name English","title":"3. Default Document Language"},{"location":"running_installation/","text":"DCR - Running - Installation Clone or copy the DCR repository from here . Switch to the file directory DCR : cd dcr Install the necessary Python packages by running the script run_dcr_prod with action m_p . Optionally, adjustments can be made in the following configuration files - details may be found here : data/db_initial_data_file.json : to configure the document languages to be used logging_cfg.yaml : for the logging functionality setup.cfg : for the DCR application in section DCR Create a PostgreSQL database container by running the script scripts/run_setup_postgresql with action prod . Create the DCR database by running the script run_dcr_prod with action db_c .","title":"Installaion"},{"location":"running_installation/#dcr-running-installation","text":"Clone or copy the DCR repository from here . Switch to the file directory DCR : cd dcr Install the necessary Python packages by running the script run_dcr_prod with action m_p . Optionally, adjustments can be made in the following configuration files - details may be found here : data/db_initial_data_file.json : to configure the document languages to be used logging_cfg.yaml : for the logging functionality setup.cfg : for the DCR application in section DCR Create a PostgreSQL database container by running the script scripts/run_setup_postgresql with action prod . Create the DCR database by running the script run_dcr_prod with action db_c .","title":"DCR - Running - Installation"},{"location":"running_operations/","text":"DCR - Running - Operations DCR should be operated via the script run_dcr_prod . The following actions are available: Action Process all Run the complete processing of all new documents. db_c Create the database. db_u Upgrade the database. m_d Run the installation of the necessary 3rd party packages for development and run the development ecosystem. m_p Run the installation of the necessary 3rd party packages for production and compile all packages and modules. n_2_p Convert non pdf documents to pdf files. ocr Convert image files to pdf files. p_2_i Convert pdf documents to image files. p_i Process the inbox directory. s_p_j Store the parser result in a JSON file. tet Extract text from pdf documents. tkn Create qualified document tokens. The action all - run the complete processing of all new documents includes the following processes in the order given: Action Process p_i Process the inbox directory. p_2_i Convert pdf documents to image files. ocr Convert image files to pdf files. n_2_p Convert non pdf documents to pdf files. tet Extract text from pdf documents. s_p_j Store the parser result in a JSON file. tkn Create qualified document tokens. The action db_c - create the database is only required once when installing DCR . The action db_u - upgrade the database is necessary once for each version change of DCR . The actions m_d and m_p correspond to the commands make pipenv-dev and make pipenv-prod for installing or updating the necessary Python libraries.","title":"Operations"},{"location":"running_operations/#dcr-running-operations","text":"DCR should be operated via the script run_dcr_prod . The following actions are available: Action Process all Run the complete processing of all new documents. db_c Create the database. db_u Upgrade the database. m_d Run the installation of the necessary 3rd party packages for development and run the development ecosystem. m_p Run the installation of the necessary 3rd party packages for production and compile all packages and modules. n_2_p Convert non pdf documents to pdf files. ocr Convert image files to pdf files. p_2_i Convert pdf documents to image files. p_i Process the inbox directory. s_p_j Store the parser result in a JSON file. tet Extract text from pdf documents. tkn Create qualified document tokens. The action all - run the complete processing of all new documents includes the following processes in the order given: Action Process p_i Process the inbox directory. p_2_i Convert pdf documents to image files. ocr Convert image files to pdf files. n_2_p Convert non pdf documents to pdf files. tet Extract text from pdf documents. s_p_j Store the parser result in a JSON file. tkn Create qualified document tokens. The action db_c - create the database is only required once when installing DCR . The action db_u - upgrade the database is necessary once for each version change of DCR . The actions m_d and m_p correspond to the commands make pipenv-dev and make pipenv-prod for installing or updating the necessary Python libraries.","title":"DCR - Running - Operations"},{"location":"running_requirements/","text":"DCR - Running - Requirements The DCR-CORE library used in DCR determines the software required for the development and operation of DCR : Operating System Ubuntu 22.04 , Windows 10 oder Windows 11 . Pandoc & TeX Live PDFlib TET Poppler Python Tesseract OCR Further details can be found here .","title":"Requirementes"},{"location":"running_requirements/#dcr-running-requirements","text":"The DCR-CORE library used in DCR determines the software required for the development and operation of DCR : Operating System Ubuntu 22.04 , Windows 10 oder Windows 11 . Pandoc & TeX Live PDFlib TET Poppler Python Tesseract OCR Further details can be found here .","title":"DCR - Running - Requirements"}]}